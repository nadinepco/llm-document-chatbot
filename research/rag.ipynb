{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('../data/Natural Language Processing with Python.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 0}),\n",
       " Document(page_content='', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 1}),\n",
       " Document(page_content='Natural Language Processing with Python', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 2}),\n",
       " Document(page_content='', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 3}),\n",
       " Document(page_content='Natural Language Processing\\nwith Python\\nSteven Bird, Ewan Klein, and Edward Loper\\nBeijing •Cambridge •Farnham •Köln •Sebastopol •Taipei •Tokyo', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 4}),\n",
       " Document(page_content='Natural Language Processing with Python\\nby Steven Bird, Ewan Klein, and Edward Loper\\nCopyright © 2009 Steven Bird, Ewan Klein, and Edward Loper. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly \\nbooks may be purchased for educational, business, or sales promotional use. Online editions\\nare also available for most titles ( http://my.safaribooksonline.com). For more information, contact our\\ncorporate/institutional sales department: (800) 998-9938 or corporate@oreilly.com.\\nEditor: Julie Steele\\nProduction Editor: Loranah Dimant\\nCopyeditor: Genevieve d’Entremont\\nProofreader: Loranah DimantIndexer: Ellen Troutman Zaig\\nCover Designer: Karen Montgomery\\nInterior Designer: David Futato\\nIllustrator: Robert Romano\\nPrinting History:\\nJune 2009:\\nFirst Edition. \\nNutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of\\nO’Reilly \\nMedia, Inc. Natural Language Processing with Python , the image of a right whale, and related\\ntrade dress are trademarks of O’Reilly Media, Inc.\\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as\\ntrademarks. Where those designations appear in this book, and O’Reilly Media, Inc. was aware of a\\ntrademark claim, the designations have been printed in caps or initial caps.\\nWhile every precaution has been taken in the preparation of this book, the publisher and authors assume\\nno responsibility for errors or omissions, or for damages resulting from the use of the information con-\\ntained herein.\\nISBN: 978-0-596-51649-9\\n[M]\\n1244726609', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 5}),\n",
       " Document(page_content='Table of Contents\\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\\n1. Language Processing and Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\n1.1 Computing with Language: Texts and Words 1\\n1.2 A Closer Look at Python: Texts as Lists of Words 10\\n1.3 Computing with Language: Simple Statistics 16\\n1.4 Back to Python: Making Decisions and Taking Control 22\\n1.5 Automatic Natural Language Understanding 27\\n1.6 Summary 33\\n1.7 Further Reading 34\\n1.8 Exercises 35\\n2. Accessing Text Corpora and Lexical Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n2.1 Accessing Text Corpora 39\\n2.2 Conditional Frequency Distributions 52\\n2.3 More Python: Reusing Code 56\\n2.4 Lexical Resources 59\\n2.5 WordNet 67\\n2.6 Summary 73\\n2.7 Further Reading 73\\n2.8 Exercises 74\\n3. Processing Raw Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\n3.1 Accessing Text from the Web and from Disk 80\\n3.2 Strings: Text Processing at the Lowest Level 87\\n3.3 Text Processing with Unicode 93\\n3.4 Regular Expressions for Detecting Word Patterns 97\\n3.5 Useful Applications of Regular Expressions 102\\n3.6 Normalizing Text 107\\n3.7 Regular Expressions for Tokenizing Text 109\\n3.8 Segmentation 112\\n3.9 Formatting: From Lists to Strings 116\\nv', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 6}),\n",
       " Document(page_content='3.10 Summary 121\\n3.11 Further Reading 122\\n3.12 Exercises 123\\n4. Writing Structured Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  129\\n4.1 Back to the Basics 130\\n4.2 Sequences 133\\n4.3 Questions of Style 138\\n4.4 Functions: The Foundation of Structured Programming 142\\n4.5 Doing More with Functions 149\\n4.6 Program Development 154\\n4.7 Algorithm Design 160\\n4.8 A Sample of Python Libraries 167\\n4.9 Summary 172\\n4.10 Further Reading 173\\n4.11 Exercises 173\\n5. Categorizing and Tagging Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  179\\n5.1 Using a Tagger 179\\n5.2 Tagged Corpora 181\\n5.3 Mapping Words to Properties Using Python Dictionaries 189\\n5.4 Automatic Tagging 198\\n5.5 N-Gram Tagging 202\\n5.6 Transformation-Based Tagging 208\\n5.7 How to Determine the Category of a Word 210\\n5.8 Summary 213\\n5.9 Further Reading 214\\n5.10 Exercises 215\\n6. Learning to Classify Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  221\\n6.1 Supervised Classification 221\\n6.2 Further Examples of Supervised Classification 233\\n6.3 Evaluation 237\\n6.4 Decision Trees 242\\n6.5 Naive Bayes Classifiers 245\\n6.6 Maximum Entropy Classifiers 250\\n6.7 Modeling Linguistic Patterns 254\\n6.8 Summary 256\\n6.9 Further Reading 256\\n6.10 Exercises 257\\n7. Extracting Information from Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  261\\n7.1 Information Extraction 261\\nvi | Table of Contents', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 7}),\n",
       " Document(page_content='7.2 Chunking 264\\n7.3 Developing and Evaluating Chunkers 270\\n7.4 Recursion in Linguistic Structure 277\\n7.5 Named Entity Recognition 281\\n7.6 Relation Extraction 284\\n7.7 Summary 285\\n7.8 Further Reading 286\\n7.9 Exercises 286\\n8. Analyzing Sentence Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  291\\n8.1 Some Grammatical Dilemmas 292\\n8.2 What’s the Use of Syntax? 295\\n8.3 Context-Free Grammar 298\\n8.4 Parsing with Context-Free Grammar 302\\n8.5 Dependencies and Dependency Grammar 310\\n8.6 Grammar Development 315\\n8.7 Summary 321\\n8.8 Further Reading 322\\n8.9 Exercises 322\\n9. Building Feature-Based Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  327\\n9.1 Grammatical Features 327\\n9.2 Processing Feature Structures 337\\n9.3 Extending a Feature-Based Grammar 344\\n9.4 Summary 356\\n9.5 Further Reading 357\\n9.6 Exercises 358\\n10. Analyzing the Meaning of Sentences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  361\\n10.1 Natural Language Understanding 361\\n10.2 Propositional Logic 368\\n10.3 First-Order Logic 372\\n10.4 The Semantics of English Sentences 385\\n10.5 Discourse Semantics 397\\n10.6 Summary 402\\n10.7 Further Reading 403\\n10.8 Exercises 404\\n11. Managing Linguistic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  407\\n11.1 Corpus Structure: A Case Study 407\\n11.2 The Life Cycle of a Corpus 412\\n11.3 Acquiring Data 416\\n11.4 Working with XML 425\\nTable of Contents | vii', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 8}),\n",
       " Document(page_content='11.5 Working with Toolbox Data 431\\n11.6 Describing Language Resources Using OLAC Metadata 435\\n11.7 Summary 437\\n11.8 Further Reading 437\\n11.9 Exercises 438\\nAfterword: The Language Challenge .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  441\\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  449\\nNLTK Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  459\\nGeneral Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  463\\nviii | Table of Contents', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 9}),\n",
       " Document(page_content='Preface\\nThis is a book about Natural Language Processing. By “natural language” we mean a\\nlanguage \\nthat is used for everyday communication by humans; languages such as Eng-\\nlish, Hindi, or Portuguese. In contrast to artificial languages such as programming lan-\\nguages and mathematical notations, natural languages have evolved as they pass from\\ngeneration to generation, and are hard to pin down with explicit rules. We will take\\nNatural Language Processing—or NLP for short—in a wide sense to cover any kind of\\ncomputer manipulation of natural language. At one extreme, it could be as simple as\\ncounting word frequencies to compare different writing styles. At the other extreme,\\nNLP involves “understanding” complete human utterances, at least to the extent of\\nbeing able to give useful responses to them.\\nTechnologies based on NLP are becoming increasingly widespread. For example,\\nphones and handheld computers support predictive text and handwriting recognition;\\nweb search engines give access to information locked up in unstructured text; machine\\ntranslation allows us to retrieve texts written in Chinese and read them in Spanish. By\\nproviding more natural human-machine interfaces, and more sophisticated access to\\nstored information, language processing has come to play a central role in the multi-\\nlingual information society.\\nThis book provides a highly accessible introduction to the field of NLP. It can be used\\nfor individual study or as the textbook for a course on natural language processing or\\ncomputational linguistics, or as a supplement to courses in artificial intelligence, text\\nmining, or corpus linguistics. The book is intensely practical, containing hundreds of\\nfully worked examples and graded exercises.\\nThe book is based on the Python programming language together with an open source\\nlibrary called the Natural Language Toolkit  (NLTK). NLTK includes extensive soft-\\nware, data, and documentation, all freely downloadable from http://www.nltk.org/.\\nDistributions are provided for Windows, Macintosh, and Unix platforms. We strongly\\nencourage you to download Python and NLTK, and try out the examples and exercises\\nalong the way.\\nix', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 10}),\n",
       " Document(page_content='Audience\\nNLP is important for scientific, economic, social, and cultural reasons. NLP is experi-\\nencing \\nrapid growth as its theories and methods are deployed in a variety of new lan-\\nguage technologies. For this reason it is important for a wide range of people to have a\\nworking knowledge of NLP. Within industry, this includes people in human-computer\\ninteraction, business information analysis, and web software development. Within\\nacademia, it includes people in areas from humanities computing and corpus linguistics\\nthrough to computer science and artificial intelligence. (To many people in academia,\\nNLP is known by the name of “Computational Linguistics.”)\\nThis book is intended for a diverse range of people who want to learn how to write\\nprograms that analyze written language, regardless of previous programming\\nexperience:\\nNew to programming?\\nThe early chapters of the book are suitable for readers with no prior knowledge of\\nprogramming, so long as you aren’t afraid to tackle new concepts and develop new\\ncomputing skills. The book is full of examples that you can copy and try for your-\\nself, together with hundreds of graded exercises. If you need a more general intro-\\nduction to Python, see the list of Python resources at http://docs.python.org/.\\nNew to Python?\\nExperienced programmers can quickly learn enough Python using this book to get\\nimmersed in natural language processing. All relevant Python features are carefully\\nexplained and exemplified, and you will quickly come to appreciate Python’s suit-\\nability for this application area. The language index will help you locate relevant\\ndiscussions in the book.\\nAlready dreaming in Python?\\nSkim the Python examples and dig into the interesting language analysis material\\nthat starts in Chapter 1 . You’ll soon be applying your skills to this fascinating\\ndomain.\\nEmphasis\\nThis book is a practical introduction to NLP. You will learn by example, write real\\nprograms, and grasp the value of being able to test an idea through implementation. If\\nyou haven’t learned already, this book will teach you programming. Unlike other\\nprogramming books, we provide extensive illustrations and exercises from NLP. The\\napproach we have taken is also principled, in that we cover the theoretical underpin-\\nnings and don’t shy away from careful linguistic and computational analysis. We have\\ntried to be pragmatic in striking a balance between theory and application, identifying\\nthe connections and the tensions. Finally, we recognize that you won’t get through this\\nunless it is also pleasurable, so we have tried to include many applications and ex-\\namples that are interesting and entertaining, and sometimes whimsical.\\nx | Preface', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 11}),\n",
       " Document(page_content='Note that this book is not a reference work. Its coverage of Python and NLP is selective,\\nand presented in a tutorial style. For reference material, please consult the substantial\\nquantity \\nof searchable resources available at http://python.org/ and http://www.nltk\\n.org/.\\nThis book is not an advanced computer science text. The content ranges from intro-\\nductory to intermediate, and is directed at readers who want to learn how to analyze\\ntext using Python and the Natural Language Toolkit. To learn about advanced algo-\\nrithms implemented in NLTK, you can examine the Python code linked from http://\\nwww.nltk.org/, and consult the other materials cited in this book.\\nWhat You Will Learn\\nBy digging into the material presented here, you will learn:\\n• How simple programs can help you manipulate and analyze language data, and\\nhow to write these programs\\n• How key concepts from NLP and linguistics are used to describe and analyze\\nlanguage\\n• How data structures and algorithms are used in NLP\\n• How language data is stored in standard formats, and how data can be used to\\nevaluate the performance of NLP techniques\\nDepending on your background, and your motivation for being interested in NLP, you\\nwill gain different kinds of skills and knowledge from this book, as set out in Table P-1.\\nTable P-1. Skills and knowledge to be gained from reading this book, depending on readers’ goals and\\nbackground\\nGoals Background in arts and humanities Background in science and engineering\\nLanguage\\nanalysisManipulating large corpora, exploring linguistic\\nmodels, and testing empirical claims.Using techniques in data modeling, data mining, and\\nknowledge discovery to analyze natural language.\\nLanguage\\ntechnologyBuilding robust systems to perform linguistic tasks\\nwith technological applications.Using linguistic algorithms and data structures in robust\\nlanguage processing software.\\nOrganization\\nThe \\nearly chapters are organized in order of conceptual difficulty, starting with a prac-\\ntical introduction to language processing that shows how to explore interesting bodies\\nof text using tiny Python programs (Chapters 1–3). This is followed by a chapter on\\nstructured programming ( Chapter 4) that consolidates the programming topics scat-\\ntered across the preceding chapters. After this, the pace picks up, and we move on to\\na series of chapters covering fundamental topics in language processing: tagging, clas-\\nsification, and information extraction (Chapters 5–7). The next three chapters look at\\nPreface | xi', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 12}),\n",
       " Document(page_content='ways to parse a sentence, recognize its syntactic structure, and construct representa-\\ntions \\nof meaning (Chapters 8–10). The final chapter is devoted to linguistic data and\\nhow it can be managed effectively ( Chapter 11 ). The book concludes with an After-\\nword, briefly discussing the past and future of the field.\\nWithin each chapter, we switch between different styles of presentation. In one style,\\nnatural language is the driver. We analyze language, explore linguistic concepts, and\\nuse programming examples to support the discussion. We often employ Python con-\\nstructs that have not been introduced systematically, so you can see their purpose before\\ndelving into the details of how and why they work. This is just like learning idiomatic\\nexpressions in a foreign language: you’re able to buy a nice pastry without first having\\nlearned the intricacies of question formation. In the other style of presentation, the\\nprogramming language will be the driver. We’ll analyze programs, explore algorithms,\\nand the linguistic examples will play a supporting role.\\nEach chapter ends with a series of graded exercises, which are useful for consolidating\\nthe material. The exercises are graded according to the following scheme: ○ is for easy\\nexercises that involve minor modifications to supplied code samples or other simple\\nactivities; ◑ is for intermediate exercises that explore an aspect of the material in more\\ndepth, requiring careful analysis and design; ● is for difficult, open-ended tasks that\\nwill challenge your understanding of the material and force you to think independently\\n(readers new to programming should skip these).\\nEach chapter has a further reading section and an online “extras” section at http://www\\n.nltk.org/, with pointers to more advanced materials and online resources. Online ver-\\nsions of all the code examples are also available there.\\nWhy Python?\\nPython is a simple yet powerful programming language with excellent functionality for\\nprocessing linguistic data. Python can be downloaded for free from http://www.python\\n.org/. Installers are available for all platforms.\\nHere is a five-line Python program that processes file.txt and prints all the words ending\\nin ing:\\n>>> for line in open(\"file.txt\"):\\n...     for word in line.split():\\n...         if word.endswith(\\'ing\\'):\\n...             print word\\nThis program illustrates some of the main features of Python. First, whitespace is used\\nto nest lines of code; thus the line starting with if falls inside the scope of the previous\\nline starting with for; this ensures that the ing test is performed for each word. Second,\\nPython is object-oriented; each variable is an entity that has certain defined attributes\\nand methods. For example, the value of the variable line is more than a sequence of\\ncharacters. It is a string object that has a “method” (or operation) called split() that\\nxii | Preface', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 13}),\n",
       " Document(page_content=\"we can use to break a line into its words. To apply a method to an object, we write the\\nobject \\nname, followed by a period, followed by the method name, i.e., line.split().\\nThird, methods have arguments expressed inside parentheses. For instance, in the ex-\\nample, word.endswith('ing') had the argument 'ing' to indicate that we wanted words\\nending with ing and not something else. Finally—and most importantly—Python is\\nhighly readable, so much so that it is fairly easy to guess what this program does even\\nif you have never written a program before.\\nWe chose Python because it has a shallow learning curve, its syntax and semantics are\\ntransparent, and it has good string-handling functionality. As an interpreted language,\\nPython facilitates interactive exploration. As an object-oriented language, Python per-\\nmits data and methods to be encapsulated and re-used easily. As a dynamic language,\\nPython permits attributes to be added to objects on the fly, and permits variables to be\\ntyped dynamically, facilitating rapid development. Python comes with an extensive\\nstandard library, including components for graphical programming, numerical pro-\\ncessing, and web connectivity.\\nPython is heavily used in industry, scientific research, and education around the world.\\nPython is often praised for the way it facilitates productivity, quality, and main-\\ntainability of software. A collection of Python success stories is posted at http://www\\n.python.org/about/success/.\\nNLTK defines an infrastructure that can be used to build NLP programs in Python. It\\nprovides basic classes for representing data relevant to natural language processing;\\nstandard interfaces for performing tasks such as part-of-speech tagging, syntactic pars-\\ning, and text classification; and standard implementations for each task that can be\\ncombined to solve complex problems.\\nNLTK comes with extensive documentation. In addition to this book, the website at\\nhttp://www.nltk.org/ provides API documentation that covers every module, class, and\\nfunction in the toolkit, specifying parameters and giving examples of usage. The website\\nalso provides many HOWTOs with extensive examples and test cases, intended for\\nusers, developers, and instructors.\\nSoftware Requirements\\nTo get the most out of this book, you should install several free software packages.\\nCurrent download pointers and instructions are available at http://www.nltk.org/.\\nPython\\nThe material presented in this book assumes that you are using Python version 2.4\\nor 2.5. We are committed to porting NLTK to Python 3.0 once the libraries that\\nNLTK depends on have been ported.\\nNLTK\\nThe code examples in this book use NLTK version 2.0. Subsequent releases of\\nNLTK will be backward-compatible.\\nPreface | xiii\", metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 14}),\n",
       " Document(page_content='NLTK-Data\\nThis contains the linguistic corpora that are analyzed and processed in the book.\\nNumPy (recommended)\\nThis \\nis a scientific computing library with support for multidimensional arrays and\\nlinear algebra, required for certain probability, tagging, clustering, and classifica-\\ntion tasks.\\nMatplotlib (recommended)\\nThis is a 2D plotting library for data visualization, and is used in some of the book’s\\ncode samples that produce line graphs and bar charts.\\nNetworkX (optional)\\nThis is a library for storing and manipulating network structures consisting of\\nnodes and edges. For visualizing semantic networks, also install the Graphviz\\nlibrary.\\nProver9 (optional)\\nThis is an automated theorem prover for first-order and equational logic, used to\\nsupport inference in language processing.\\nNatural Language Toolkit (NLTK)\\nNLTK was originally created in 2001 as part of a computational linguistics course in\\nthe Department of Computer and Information Science at the University of Pennsylva-\\nnia. Since then it has been developed and expanded with the help of dozens of con-\\ntributors. It has now been adopted in courses in dozens of universities, and serves as\\nthe basis of many research projects. Table P-2  lists the most important NLTK modules.\\nTable P-2. Language processing tasks and corresponding NLTK modules with examples of\\nfunctionality\\nLanguage processing task NLTK modules Functionality\\nAccessing corpora nltk.corpus Standardized interfaces to corpora and lexicons\\nString processing nltk.tokenize, nltk.stem Tokenizers, sentence tokenizers, stemmers\\nCollocation discovery nltk.collocations t-test, chi-squared, point-wise mutual information\\nPart-of-speech tagging nltk.tag n-gram, backoff, Brill, HMM, TnT\\nClassification nltk.classify, nltk.cluster Decision tree, maximum entropy, naive Bayes, EM, k-means\\nChunking nltk.chunk Regular expression, n-gram, named entity\\nParsing nltk.parse Chart, feature-based, unification, probabilistic, dependency\\nSemantic interpretation nltk.sem, nltk.inference Lambda calculus, first-order logic, model checking\\nEvaluation metrics nltk.metrics Precision, recall, agreement coefficients\\nProbability and estimation nltk.probability Frequency distributions, smoothed probability distributions\\nApplications nltk.app, nltk.chat Graphical concordancer, parsers, WordNet browser, chatbots\\nxiv | Preface', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 15}),\n",
       " Document(page_content='Language processing task NLTK modules Functionality\\nLinguistic fieldwork nltk.toolbox Manipulate data in SIL Toolbox format\\nNLTK was designed with four primary goals in mind:\\nSimplicity\\nTo \\nprovide an intuitive framework along with substantial building blocks, giving\\nusers a practical knowledge of NLP without getting bogged down in the tedious\\nhouse-keeping usually associated with processing annotated language data\\nConsistency\\nTo provide a uniform framework with consistent interfaces and data structures,\\nand easily guessable method names\\nExtensibility\\nTo provide a structure into which new software modules can be easily accommo-\\ndated, including alternative implementations and competing approaches to the\\nsame task\\nModularity\\nTo provide components that can be used independently without needing to un-\\nderstand the rest of the toolkit\\nContrasting with these goals are three non-requirements—potentially useful qualities\\nthat we have deliberately avoided. First, while the toolkit provides a wide range of\\nfunctions, it is not encyclopedic; it is a toolkit, not a system, and it will continue to\\nevolve with the field of NLP. Second, while the toolkit is efficient enough to support\\nmeaningful tasks, it is not highly optimized for runtime performance; such optimiza-\\ntions often involve more complex algorithms, or implementations in lower-level pro-\\ngramming languages such as C or C++. This would make the software less readable\\nand more difficult to install. Third, we have tried to avoid clever programming tricks,\\nsince we believe that clear implementations are preferable to ingenious yet indecipher-\\nable ones.\\nFor Instructors\\nNatural Language Processing is often taught within the confines of a single-semester\\ncourse at the advanced undergraduate level or postgraduate level. Many instructors\\nhave found that it is difficult to cover both the theoretical and practical sides of the\\nsubject in such a short span of time. Some courses focus on theory to the exclusion of\\npractical exercises, and deprive students of the challenge and excitement of writing\\nprograms to automatically process language. Other courses are simply designed to\\nteach programming for linguists, and do not manage to cover any significant NLP con-\\ntent. NLTK was originally developed to address this problem, making it feasible to\\ncover a substantial amount of theory and practice within a single-semester course, even\\nif students have no prior programming experience.\\nPreface | xv', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 16}),\n",
       " Document(page_content='A significant fraction of any NLP syllabus deals with algorithms and data structures.\\nOn \\ntheir own these can be rather dry, but NLTK brings them to life with the help of\\ninteractive graphical user interfaces that make it possible to view algorithms step-by-\\nstep. Most NLTK components include a demonstration that performs an interesting\\ntask without requiring any special input from the user. An effective way to deliver the\\nmaterials is through interactive presentation of the examples in this book, entering\\nthem in a Python session, observing what they do, and modifying them to explore some\\nempirical or theoretical issue.\\nThis book contains hundreds of exercises that can be used as the basis for student\\nassignments. The simplest exercises involve modifying a supplied program fragment in\\na specified way in order to answer a concrete question. At the other end of the spectrum,\\nNLTK provides a flexible framework for graduate-level research projects, with standard\\nimplementations of all the basic data structures and algorithms, interfaces to dozens\\nof widely used datasets (corpora), and a flexible and extensible architecture. Additional\\nsupport for teaching using NLTK is available on the NLTK website.\\nWe believe this book is unique in providing a comprehensive framework for students\\nto learn about NLP in the context of learning to program. What sets these materials\\napart is the tight coupling of the chapters and exercises with NLTK, giving students—\\neven those with no prior programming experience—a practical introduction to NLP.\\nAfter completing these materials, students will be ready to attempt one of the more\\nadvanced textbooks, such as Speech and Language Processing , by Jurafsky and Martin\\n(Prentice Hall, 2008).\\nThis book presents programming concepts in an unusual order, beginning with a non-\\ntrivial data type—lists of strings—then introducing non-trivial control structures such\\nas comprehensions and conditionals. These idioms permit us to do useful language\\nprocessing from the start. Once this motivation is in place, we return to a systematic\\npresentation of fundamental concepts such as strings, loops, files, and so forth. In this\\nway, we cover the same ground as more conventional approaches, without expecting\\nreaders to be interested in the programming language for its own sake.\\nTwo possible course plans are illustrated in Table P-3 . The first one presumes an arts/\\nhumanities audience, whereas the second one presumes a science/engineering audi-\\nence. Other course plans could cover the first five chapters, then devote the remaining\\ntime to a single area, such as text classification (Chapters 6 and 7), syntax (Chapters\\n8 and 9), semantics (Chapter 10), or linguistic data management (Chapter 11).\\nTable P-3. Suggested course plans; approximate number of lectures per chapter\\nChapter Arts and Humanities Science and Engineering\\nChapter 1, Language Processing and Python 2–4 2\\nChapter 2, Accessing Text Corpora and Lexical Resources 2–4 2\\nChapter 3, Processing Raw Text 2–4 2\\nChapter 4, Writing Structured Programs 2–4 1–2\\nxvi | Preface', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 17}),\n",
       " Document(page_content='Chapter Arts and Humanities Science and Engineering\\nChapter 5, Categorizing and Tagging Words 2–4 2–4\\nChapter 6, Learning to Classify Text 0–2 2–4\\nChapter 7, Extracting Information from Text 2 2–4\\nChapter 8, Analyzing Sentence Structure 2–4 2–4\\nChapter 9, Building Feature-Based Grammars 2–4 1–4\\nChapter 10, Analyzing the Meaning of Sentences 1–2 1–4\\nChapter 11, Managing Linguistic Data 1–2 1–4\\nTotal 18–36 18–36\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nBold\\nIndicates new terms.\\nItalic\\nUsed \\nwithin paragraphs to refer to linguistic examples, the names of texts, and\\nURLs; also used for filenames and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program elements\\nsuch as variable or function names, statements, and keywords; also used for pro-\\ngram names.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter-\\nmined by context; also used for metavariables within program code examples.\\nThis icon signifies a tip, suggestion, or general note.\\nThis icon indicates a warning or caution.\\nUsing Code Examples\\nThis \\nbook is here to help you get your job done. In general, you may use the code in\\nthis book in your programs and documentation. You do not need to contact us for\\npermission unless you’re reproducing a significant portion of the code. For example,\\nPreface | xvii', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 18}),\n",
       " Document(page_content='writing a program that uses several chunks of code from this book does not require\\npermission. \\nSelling or distributing a CD-ROM of examples from O’Reilly books does\\nrequire permission. Answering a question by citing this book and quoting example\\ncode does not require permission. Incorporating a significant amount of example code\\nfrom this book into your product’s documentation does require permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the title,\\nauthor, publisher, and ISBN. For example: “ Natural Language Processing with Py-\\nthon, by Steven Bird, Ewan Klein, and Edward Loper. Copyright 2009 Steven Bird,\\nEwan Klein, and Edward Loper, 978-0-596-51649-9.”\\nIf you feel your use of code examples falls outside fair use or the permission given above,\\nfeel free to contact us at permissions@oreilly.com.\\nSafari® Books Online\\nWhen you see a Safari® Books Online icon on the cover of your favorite\\ntechnology \\nbook, that means the book is available online through the\\nO’Reilly Network Safari Bookshelf.\\nSafari offers a solution that’s better than e-books. It’s a virtual library that lets you easily\\nsearch thousands of top tech books, cut and paste code samples, download chapters,\\nand find quick answers when you need the most accurate, current information. Try it\\nfor free at http://my.safaribooksonline.com.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at:\\nhttp://www.oreilly.com/catalog/9780596516499\\nxviii | Preface', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len)\n",
    "doc_chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Natural Language Processing with Python', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 2}),\n",
       " Document(page_content='Natural Language Processing\\nwith Python\\nSteven Bird, Ewan Klein, and Edward Loper\\nBeijing •Cambridge •Farnham •Köln •Sebastopol •Taipei •Tokyo', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 4}),\n",
       " Document(page_content='Natural Language Processing with Python\\nby Steven Bird, Ewan Klein, and Edward Loper\\nCopyright © 2009 Steven Bird, Ewan Klein, and Edward Loper. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly \\nbooks may be purchased for educational, business, or sales promotional use. Online editions\\nare also available for most titles ( http://my.safaribooksonline.com). For more information, contact our\\ncorporate/institutional sales department: (800) 998-9938 or corporate@oreilly.com.\\nEditor: Julie Steele\\nProduction Editor: Loranah Dimant\\nCopyeditor: Genevieve d’Entremont\\nProofreader: Loranah DimantIndexer: Ellen Troutman Zaig\\nCover Designer: Karen Montgomery\\nInterior Designer: David Futato\\nIllustrator: Robert Romano\\nPrinting History:\\nJune 2009:\\nFirst Edition. \\nNutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of\\nO’Reilly', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 5}),\n",
       " Document(page_content='Illustrator: Robert Romano\\nPrinting History:\\nJune 2009:\\nFirst Edition. \\nNutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of\\nO’Reilly \\nMedia, Inc. Natural Language Processing with Python , the image of a right whale, and related\\ntrade dress are trademarks of O’Reilly Media, Inc.\\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as\\ntrademarks. Where those designations appear in this book, and O’Reilly Media, Inc. was aware of a\\ntrademark claim, the designations have been printed in caps or initial caps.\\nWhile every precaution has been taken in the preparation of this book, the publisher and authors assume\\nno responsibility for errors or omissions, or for damages resulting from the use of the information con-\\ntained herein.\\nISBN: 978-0-596-51649-9\\n[M]\\n1244726609', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 5}),\n",
       " Document(page_content='Table of Contents\\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\\n1. Language Processing and Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\n1.1 Computing with Language: Texts and Words 1\\n1.2 A Closer Look at Python: Texts as Lists of Words 10\\n1.3 Computing with Language: Simple Statistics 16\\n1.4 Back to Python: Making Decisions and Taking Control 22\\n1.5 Automatic Natural Language Understanding 27\\n1.6 Summary 33\\n1.7 Further Reading 34\\n1.8 Exercises 35\\n2. Accessing Text Corpora and Lexical Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n2.1 Accessing Text Corpora 39\\n2.2 Conditional Frequency Distributions 52\\n2.3 More Python: Reusing Code 56\\n2.4 Lexical Resources 59\\n2.5 WordNet 67\\n2.6 Summary 73\\n2.7 Further Reading 73\\n2.8 Exercises 74', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 6}),\n",
       " Document(page_content='2.2 Conditional Frequency Distributions 52\\n2.3 More Python: Reusing Code 56\\n2.4 Lexical Resources 59\\n2.5 WordNet 67\\n2.6 Summary 73\\n2.7 Further Reading 73\\n2.8 Exercises 74\\n3. Processing Raw Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\n3.1 Accessing Text from the Web and from Disk 80\\n3.2 Strings: Text Processing at the Lowest Level 87\\n3.3 Text Processing with Unicode 93\\n3.4 Regular Expressions for Detecting Word Patterns 97\\n3.5 Useful Applications of Regular Expressions 102\\n3.6 Normalizing Text 107\\n3.7 Regular Expressions for Tokenizing Text 109\\n3.8 Segmentation 112\\n3.9 Formatting: From Lists to Strings 116\\nv', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 6}),\n",
       " Document(page_content='3.10 Summary 121\\n3.11 Further Reading 122\\n3.12 Exercises 123\\n4. Writing Structured Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  129\\n4.1 Back to the Basics 130\\n4.2 Sequences 133\\n4.3 Questions of Style 138\\n4.4 Functions: The Foundation of Structured Programming 142\\n4.5 Doing More with Functions 149\\n4.6 Program Development 154\\n4.7 Algorithm Design 160\\n4.8 A Sample of Python Libraries 167\\n4.9 Summary 172\\n4.10 Further Reading 173\\n4.11 Exercises 173\\n5. Categorizing and Tagging Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  179\\n5.1 Using a Tagger 179\\n5.2 Tagged Corpora 181\\n5.3 Mapping Words to Properties Using Python Dictionaries 189\\n5.4 Automatic Tagging 198\\n5.5 N-Gram Tagging 202\\n5.6 Transformation-Based Tagging 208\\n5.7 How to Determine the Category of a Word 210\\n5.8 Summary 213\\n5.9 Further Reading 214\\n5.10 Exercises 215', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 7}),\n",
       " Document(page_content='5.4 Automatic Tagging 198\\n5.5 N-Gram Tagging 202\\n5.6 Transformation-Based Tagging 208\\n5.7 How to Determine the Category of a Word 210\\n5.8 Summary 213\\n5.9 Further Reading 214\\n5.10 Exercises 215\\n6. Learning to Classify Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  221\\n6.1 Supervised Classification 221\\n6.2 Further Examples of Supervised Classification 233\\n6.3 Evaluation 237\\n6.4 Decision Trees 242\\n6.5 Naive Bayes Classifiers 245\\n6.6 Maximum Entropy Classifiers 250\\n6.7 Modeling Linguistic Patterns 254\\n6.8 Summary 256\\n6.9 Further Reading 256\\n6.10 Exercises 257\\n7. Extracting Information from Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  261\\n7.1 Information Extraction 261\\nvi | Table of Contents', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 7}),\n",
       " Document(page_content='7.2 Chunking 264\\n7.3 Developing and Evaluating Chunkers 270\\n7.4 Recursion in Linguistic Structure 277\\n7.5 Named Entity Recognition 281\\n7.6 Relation Extraction 284\\n7.7 Summary 285\\n7.8 Further Reading 286\\n7.9 Exercises 286\\n8. Analyzing Sentence Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  291\\n8.1 Some Grammatical Dilemmas 292\\n8.2 What’s the Use of Syntax? 295\\n8.3 Context-Free Grammar 298\\n8.4 Parsing with Context-Free Grammar 302\\n8.5 Dependencies and Dependency Grammar 310\\n8.6 Grammar Development 315\\n8.7 Summary 321\\n8.8 Further Reading 322\\n8.9 Exercises 322\\n9. Building Feature-Based Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  327\\n9.1 Grammatical Features 327\\n9.2 Processing Feature Structures 337\\n9.3 Extending a Feature-Based Grammar 344\\n9.4 Summary 356\\n9.5 Further Reading 357\\n9.6 Exercises 358', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 8}),\n",
       " Document(page_content='9.1 Grammatical Features 327\\n9.2 Processing Feature Structures 337\\n9.3 Extending a Feature-Based Grammar 344\\n9.4 Summary 356\\n9.5 Further Reading 357\\n9.6 Exercises 358\\n10. Analyzing the Meaning of Sentences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  361\\n10.1 Natural Language Understanding 361\\n10.2 Propositional Logic 368\\n10.3 First-Order Logic 372\\n10.4 The Semantics of English Sentences 385\\n10.5 Discourse Semantics 397\\n10.6 Summary 402\\n10.7 Further Reading 403\\n10.8 Exercises 404\\n11. Managing Linguistic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  407\\n11.1 Corpus Structure: A Case Study 407\\n11.2 The Life Cycle of a Corpus 412\\n11.3 Acquiring Data 416\\n11.4 Working with XML 425\\nTable of Contents | vii', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 8}),\n",
       " Document(page_content='11.5 Working with Toolbox Data 431\\n11.6 Describing Language Resources Using OLAC Metadata 435\\n11.7 Summary 437\\n11.8 Further Reading 437\\n11.9 Exercises 438\\nAfterword: The Language Challenge .\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  441\\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  449\\nNLTK Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  459\\nGeneral Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  463\\nviii | Table of Contents', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 9}),\n",
       " Document(page_content='Preface\\nThis is a book about Natural Language Processing. By “natural language” we mean a\\nlanguage \\nthat is used for everyday communication by humans; languages such as Eng-\\nlish, Hindi, or Portuguese. In contrast to artificial languages such as programming lan-\\nguages and mathematical notations, natural languages have evolved as they pass from\\ngeneration to generation, and are hard to pin down with explicit rules. We will take\\nNatural Language Processing—or NLP for short—in a wide sense to cover any kind of\\ncomputer manipulation of natural language. At one extreme, it could be as simple as\\ncounting word frequencies to compare different writing styles. At the other extreme,\\nNLP involves “understanding” complete human utterances, at least to the extent of\\nbeing able to give useful responses to them.\\nTechnologies based on NLP are becoming increasingly widespread. For example,\\nphones and handheld computers support predictive text and handwriting recognition;', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 10}),\n",
       " Document(page_content='Technologies based on NLP are becoming increasingly widespread. For example,\\nphones and handheld computers support predictive text and handwriting recognition;\\nweb search engines give access to information locked up in unstructured text; machine\\ntranslation allows us to retrieve texts written in Chinese and read them in Spanish. By\\nproviding more natural human-machine interfaces, and more sophisticated access to\\nstored information, language processing has come to play a central role in the multi-\\nlingual information society.\\nThis book provides a highly accessible introduction to the field of NLP. It can be used\\nfor individual study or as the textbook for a course on natural language processing or\\ncomputational linguistics, or as a supplement to courses in artificial intelligence, text\\nmining, or corpus linguistics. The book is intensely practical, containing hundreds of\\nfully worked examples and graded exercises.', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 10}),\n",
       " Document(page_content='mining, or corpus linguistics. The book is intensely practical, containing hundreds of\\nfully worked examples and graded exercises.\\nThe book is based on the Python programming language together with an open source\\nlibrary called the Natural Language Toolkit  (NLTK). NLTK includes extensive soft-\\nware, data, and documentation, all freely downloadable from http://www.nltk.org/.\\nDistributions are provided for Windows, Macintosh, and Unix platforms. We strongly\\nencourage you to download Python and NLTK, and try out the examples and exercises\\nalong the way.\\nix', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 10}),\n",
       " Document(page_content='Audience\\nNLP is important for scientific, economic, social, and cultural reasons. NLP is experi-\\nencing \\nrapid growth as its theories and methods are deployed in a variety of new lan-\\nguage technologies. For this reason it is important for a wide range of people to have a\\nworking knowledge of NLP. Within industry, this includes people in human-computer\\ninteraction, business information analysis, and web software development. Within\\nacademia, it includes people in areas from humanities computing and corpus linguistics\\nthrough to computer science and artificial intelligence. (To many people in academia,\\nNLP is known by the name of “Computational Linguistics.”)\\nThis book is intended for a diverse range of people who want to learn how to write\\nprograms that analyze written language, regardless of previous programming\\nexperience:\\nNew to programming?\\nThe early chapters of the book are suitable for readers with no prior knowledge of', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 11}),\n",
       " Document(page_content='programs that analyze written language, regardless of previous programming\\nexperience:\\nNew to programming?\\nThe early chapters of the book are suitable for readers with no prior knowledge of\\nprogramming, so long as you aren’t afraid to tackle new concepts and develop new\\ncomputing skills. The book is full of examples that you can copy and try for your-\\nself, together with hundreds of graded exercises. If you need a more general intro-\\nduction to Python, see the list of Python resources at http://docs.python.org/.\\nNew to Python?\\nExperienced programmers can quickly learn enough Python using this book to get\\nimmersed in natural language processing. All relevant Python features are carefully\\nexplained and exemplified, and you will quickly come to appreciate Python’s suit-\\nability for this application area. The language index will help you locate relevant\\ndiscussions in the book.\\nAlready dreaming in Python?\\nSkim the Python examples and dig into the interesting language analysis material', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 11}),\n",
       " Document(page_content='discussions in the book.\\nAlready dreaming in Python?\\nSkim the Python examples and dig into the interesting language analysis material\\nthat starts in Chapter 1 . You’ll soon be applying your skills to this fascinating\\ndomain.\\nEmphasis\\nThis book is a practical introduction to NLP. You will learn by example, write real\\nprograms, and grasp the value of being able to test an idea through implementation. If\\nyou haven’t learned already, this book will teach you programming. Unlike other\\nprogramming books, we provide extensive illustrations and exercises from NLP. The\\napproach we have taken is also principled, in that we cover the theoretical underpin-\\nnings and don’t shy away from careful linguistic and computational analysis. We have\\ntried to be pragmatic in striking a balance between theory and application, identifying\\nthe connections and the tensions. Finally, we recognize that you won’t get through this\\nunless it is also pleasurable, so we have tried to include many applications and ex-', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 11}),\n",
       " Document(page_content='the connections and the tensions. Finally, we recognize that you won’t get through this\\nunless it is also pleasurable, so we have tried to include many applications and ex-\\namples that are interesting and entertaining, and sometimes whimsical.\\nx | Preface', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 11}),\n",
       " Document(page_content='Note that this book is not a reference work. Its coverage of Python and NLP is selective,\\nand presented in a tutorial style. For reference material, please consult the substantial\\nquantity \\nof searchable resources available at http://python.org/ and http://www.nltk\\n.org/.\\nThis book is not an advanced computer science text. The content ranges from intro-\\nductory to intermediate, and is directed at readers who want to learn how to analyze\\ntext using Python and the Natural Language Toolkit. To learn about advanced algo-\\nrithms implemented in NLTK, you can examine the Python code linked from http://\\nwww.nltk.org/, and consult the other materials cited in this book.\\nWhat You Will Learn\\nBy digging into the material presented here, you will learn:\\n• How simple programs can help you manipulate and analyze language data, and\\nhow to write these programs\\n• How key concepts from NLP and linguistics are used to describe and analyze\\nlanguage\\n• How data structures and algorithms are used in NLP', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 12}),\n",
       " Document(page_content='how to write these programs\\n• How key concepts from NLP and linguistics are used to describe and analyze\\nlanguage\\n• How data structures and algorithms are used in NLP\\n• How language data is stored in standard formats, and how data can be used to\\nevaluate the performance of NLP techniques\\nDepending on your background, and your motivation for being interested in NLP, you\\nwill gain different kinds of skills and knowledge from this book, as set out in Table P-1.\\nTable P-1. Skills and knowledge to be gained from reading this book, depending on readers’ goals and\\nbackground\\nGoals Background in arts and humanities Background in science and engineering\\nLanguage\\nanalysisManipulating large corpora, exploring linguistic\\nmodels, and testing empirical claims.Using techniques in data modeling, data mining, and\\nknowledge discovery to analyze natural language.\\nLanguage\\ntechnologyBuilding robust systems to perform linguistic tasks', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 12})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadineco/miniconda3/envs/llm/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "## FAISS vector database\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db=FAISS.from_documents(doc_chunks[:30], OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector database\n",
    "query=\"What is NLTK?\"\n",
    "result = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='NLTK-Data\\nThis contains the linguistic corpora that are analyzed and processed in the book.\\nNumPy (recommended)\\nThis \\nis a scientific computing library with support for multidimensional arrays and\\nlinear algebra, required for certain probability, tagging, clustering, and classifica-\\ntion tasks.\\nMatplotlib (recommended)\\nThis is a 2D plotting library for data visualization, and is used in some of the book’s\\ncode samples that produce line graphs and bar charts.\\nNetworkX (optional)\\nThis is a library for storing and manipulating network structures consisting of\\nnodes and edges. For visualizing semantic networks, also install the Graphviz\\nlibrary.\\nProver9 (optional)\\nThis is an automated theorem prover for first-order and equational logic, used to\\nsupport inference in language processing.\\nNatural Language Toolkit (NLTK)\\nNLTK was originally created in 2001 as part of a computational linguistics course in\\nthe Department of Computer and Information Science at the University of Pennsylva-', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 15}),\n",
       " Document(page_content='standard interfaces for performing tasks such as part-of-speech tagging, syntactic pars-\\ning, and text classification; and standard implementations for each task that can be\\ncombined to solve complex problems.\\nNLTK comes with extensive documentation. In addition to this book, the website at\\nhttp://www.nltk.org/ provides API documentation that covers every module, class, and\\nfunction in the toolkit, specifying parameters and giving examples of usage. The website\\nalso provides many HOWTOs with extensive examples and test cases, intended for\\nusers, developers, and instructors.\\nSoftware Requirements\\nTo get the most out of this book, you should install several free software packages.\\nCurrent download pointers and instructions are available at http://www.nltk.org/.\\nPython\\nThe material presented in this book assumes that you are using Python version 2.4\\nor 2.5. We are committed to porting NLTK to Python 3.0 once the libraries that\\nNLTK depends on have been ported.\\nNLTK', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 14}),\n",
       " Document(page_content='Note that this book is not a reference work. Its coverage of Python and NLP is selective,\\nand presented in a tutorial style. For reference material, please consult the substantial\\nquantity \\nof searchable resources available at http://python.org/ and http://www.nltk\\n.org/.\\nThis book is not an advanced computer science text. The content ranges from intro-\\nductory to intermediate, and is directed at readers who want to learn how to analyze\\ntext using Python and the Natural Language Toolkit. To learn about advanced algo-\\nrithms implemented in NLTK, you can examine the Python code linked from http://\\nwww.nltk.org/, and consult the other materials cited in this book.\\nWhat You Will Learn\\nBy digging into the material presented here, you will learn:\\n• How simple programs can help you manipulate and analyze language data, and\\nhow to write these programs\\n• How key concepts from NLP and linguistics are used to describe and analyze\\nlanguage\\n• How data structures and algorithms are used in NLP', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 12}),\n",
       " Document(page_content='mining, or corpus linguistics. The book is intensely practical, containing hundreds of\\nfully worked examples and graded exercises.\\nThe book is based on the Python programming language together with an open source\\nlibrary called the Natural Language Toolkit  (NLTK). NLTK includes extensive soft-\\nware, data, and documentation, all freely downloadable from http://www.nltk.org/.\\nDistributions are provided for Windows, Macintosh, and Unix platforms. We strongly\\nencourage you to download Python and NLTK, and try out the examples and exercises\\nalong the way.\\nix', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chain and Retriever\n",
    "from langchain_community.llms import Ollama\n",
    "## Load Ollama LAMA2 LLM model\n",
    "llm=Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "## load the Groq API key\n",
    "groq_api_key=os.environ['GROQ_API_KEY']\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,\n",
    "             model_name=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Introduction\n",
    "## Create Stuff Document Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10c5b1c50>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers: A retriever is an interface that returns documents given\n",
    " an unstructured query. It is more general than a vector store.\n",
    " A retriever does not need to be able to store documents, only to \n",
    " return (or retrieve) them. Vector stores can be used as the backbone\n",
    " of a retriever, but there are other types of retrievers as well. \n",
    " https://python.langchain.com/docs/modules/data_connection/retrievers/   \n",
    "\"\"\"\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieval chain:This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents \n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\"\"\"\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10c5b1c50>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based only on the provided context.\\nThink step by step before providing a detailed answer.\\n<context>\\n{context}\\n</context>\\nQuestion: {input}\\n'))])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x117397450>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x117612510>, model_name='llama3-8b-8192', groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is NLTK?',\n",
       " 'context': [Document(page_content='NLTK-Data\\nThis contains the linguistic corpora that are analyzed and processed in the book.\\nNumPy (recommended)\\nThis \\nis a scientific computing library with support for multidimensional arrays and\\nlinear algebra, required for certain probability, tagging, clustering, and classifica-\\ntion tasks.\\nMatplotlib (recommended)\\nThis is a 2D plotting library for data visualization, and is used in some of the book’s\\ncode samples that produce line graphs and bar charts.\\nNetworkX (optional)\\nThis is a library for storing and manipulating network structures consisting of\\nnodes and edges. For visualizing semantic networks, also install the Graphviz\\nlibrary.\\nProver9 (optional)\\nThis is an automated theorem prover for first-order and equational logic, used to\\nsupport inference in language processing.\\nNatural Language Toolkit (NLTK)\\nNLTK was originally created in 2001 as part of a computational linguistics course in\\nthe Department of Computer and Information Science at the University of Pennsylva-', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 15}),\n",
       "  Document(page_content='standard interfaces for performing tasks such as part-of-speech tagging, syntactic pars-\\ning, and text classification; and standard implementations for each task that can be\\ncombined to solve complex problems.\\nNLTK comes with extensive documentation. In addition to this book, the website at\\nhttp://www.nltk.org/ provides API documentation that covers every module, class, and\\nfunction in the toolkit, specifying parameters and giving examples of usage. The website\\nalso provides many HOWTOs with extensive examples and test cases, intended for\\nusers, developers, and instructors.\\nSoftware Requirements\\nTo get the most out of this book, you should install several free software packages.\\nCurrent download pointers and instructions are available at http://www.nltk.org/.\\nPython\\nThe material presented in this book assumes that you are using Python version 2.4\\nor 2.5. We are committed to porting NLTK to Python 3.0 once the libraries that\\nNLTK depends on have been ported.\\nNLTK', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 14}),\n",
       "  Document(page_content='Note that this book is not a reference work. Its coverage of Python and NLP is selective,\\nand presented in a tutorial style. For reference material, please consult the substantial\\nquantity \\nof searchable resources available at http://python.org/ and http://www.nltk\\n.org/.\\nThis book is not an advanced computer science text. The content ranges from intro-\\nductory to intermediate, and is directed at readers who want to learn how to analyze\\ntext using Python and the Natural Language Toolkit. To learn about advanced algo-\\nrithms implemented in NLTK, you can examine the Python code linked from http://\\nwww.nltk.org/, and consult the other materials cited in this book.\\nWhat You Will Learn\\nBy digging into the material presented here, you will learn:\\n• How simple programs can help you manipulate and analyze language data, and\\nhow to write these programs\\n• How key concepts from NLP and linguistics are used to describe and analyze\\nlanguage\\n• How data structures and algorithms are used in NLP', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 12}),\n",
       "  Document(page_content='mining, or corpus linguistics. The book is intensely practical, containing hundreds of\\nfully worked examples and graded exercises.\\nThe book is based on the Python programming language together with an open source\\nlibrary called the Natural Language Toolkit  (NLTK). NLTK includes extensive soft-\\nware, data, and documentation, all freely downloadable from http://www.nltk.org/.\\nDistributions are provided for Windows, Macintosh, and Unix platforms. We strongly\\nencourage you to download Python and NLTK, and try out the examples and exercises\\nalong the way.\\nix', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 10})],\n",
       " 'answer': 'According to the provided context, NLTK stands for Natural Language Toolkit. It is a library that was originally created in 2001 and provides standard interfaces for performing tasks such as part-of-speech tagging, syntactic parsing, and text classification; and standard implementations for each task that can be combined to solve complex problems.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\"input\":\"What is NLTK?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = retrieval_chain.invoke({\"input\":\"What is NLTK?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLTK-Data\\nThis contains the linguistic corpora that are analyzed and processed in the book.\\nNumPy (recommended)\\nThis \\nis a scientific computing library with support for multidimensional arrays and\\nlinear algebra, required for certain probability, tagging, clustering, and classifica-\\ntion tasks.\\nMatplotlib (recommended)\\nThis is a 2D plotting library for data visualization, and is used in some of the book’s\\ncode samples that produce line graphs and bar charts.\\nNetworkX (optional)\\nThis is a library for storing and manipulating network structures consisting of\\nnodes and edges. For visualizing semantic networks, also install the Graphviz\\nlibrary.\\nProver9 (optional)\\nThis is an automated theorem prover for first-order and equational logic, used to\\nsupport inference in language processing.\\nNatural Language Toolkit (NLTK)\\nNLTK was originally created in 2001 as part of a computational linguistics course in\\nthe Department of Computer and Information Science at the University of Pennsylva-'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['context'][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain,create_history_aware_retriever\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Think step by step before providing a detailed answer. \"\n",
    "    \"Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10c5b1c50>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "           | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x117397450>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x117612510>, model_name='llama3-8b-8192', groq_api_key=SecretStr('**********'))\n",
       "           | StrOutputParser()\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10c5b1c50>)), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Think step by step before providing a detailed answer. Use three sentences maximum and keep the answer concise.\\n\\n{context}\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x117397450>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x117612510>, model_name='llama3-8b-8192', groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is NLTK?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='NLTK-Data\\nThis contains the linguistic corpora that are analyzed and processed in the book.\\nNumPy (recommended)\\nThis \\nis a scientific computing library with support for multidimensional arrays and\\nlinear algebra, required for certain probability, tagging, clustering, and classifica-\\ntion tasks.\\nMatplotlib (recommended)\\nThis is a 2D plotting library for data visualization, and is used in some of the book’s\\ncode samples that produce line graphs and bar charts.\\nNetworkX (optional)\\nThis is a library for storing and manipulating network structures consisting of\\nnodes and edges. For visualizing semantic networks, also install the Graphviz\\nlibrary.\\nProver9 (optional)\\nThis is an automated theorem prover for first-order and equational logic, used to\\nsupport inference in language processing.\\nNatural Language Toolkit (NLTK)\\nNLTK was originally created in 2001 as part of a computational linguistics course in\\nthe Department of Computer and Information Science at the University of Pennsylva-', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 15}),\n",
       "  Document(page_content='standard interfaces for performing tasks such as part-of-speech tagging, syntactic pars-\\ning, and text classification; and standard implementations for each task that can be\\ncombined to solve complex problems.\\nNLTK comes with extensive documentation. In addition to this book, the website at\\nhttp://www.nltk.org/ provides API documentation that covers every module, class, and\\nfunction in the toolkit, specifying parameters and giving examples of usage. The website\\nalso provides many HOWTOs with extensive examples and test cases, intended for\\nusers, developers, and instructors.\\nSoftware Requirements\\nTo get the most out of this book, you should install several free software packages.\\nCurrent download pointers and instructions are available at http://www.nltk.org/.\\nPython\\nThe material presented in this book assumes that you are using Python version 2.4\\nor 2.5. We are committed to porting NLTK to Python 3.0 once the libraries that\\nNLTK depends on have been ported.\\nNLTK', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 14}),\n",
       "  Document(page_content='Note that this book is not a reference work. Its coverage of Python and NLP is selective,\\nand presented in a tutorial style. For reference material, please consult the substantial\\nquantity \\nof searchable resources available at http://python.org/ and http://www.nltk\\n.org/.\\nThis book is not an advanced computer science text. The content ranges from intro-\\nductory to intermediate, and is directed at readers who want to learn how to analyze\\ntext using Python and the Natural Language Toolkit. To learn about advanced algo-\\nrithms implemented in NLTK, you can examine the Python code linked from http://\\nwww.nltk.org/, and consult the other materials cited in this book.\\nWhat You Will Learn\\nBy digging into the material presented here, you will learn:\\n• How simple programs can help you manipulate and analyze language data, and\\nhow to write these programs\\n• How key concepts from NLP and linguistics are used to describe and analyze\\nlanguage\\n• How data structures and algorithms are used in NLP', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 12}),\n",
       "  Document(page_content='mining, or corpus linguistics. The book is intensely practical, containing hundreds of\\nfully worked examples and graded exercises.\\nThe book is based on the Python programming language together with an open source\\nlibrary called the Natural Language Toolkit  (NLTK). NLTK includes extensive soft-\\nware, data, and documentation, all freely downloadable from http://www.nltk.org/.\\nDistributions are provided for Windows, Macintosh, and Unix platforms. We strongly\\nencourage you to download Python and NLTK, and try out the examples and exercises\\nalong the way.\\nix', metadata={'source': '../data/Natural Language Processing with Python.pdf', 'page': 10})],\n",
       " 'answer': 'NLTK, or Natural Language Toolkit, is a library for natural language processing tasks, providing standard interfaces for tasks such as part-of-speech tagging, syntactic parsing, and text classification, along with standard implementations for each task.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is NLTK?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
